{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CodeLab_2_SoftmaxClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balezz/cv_course_fa_mag/blob/main/CodeLab_2_SoftmaxClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "PZ9OL7QjDAuV"
      },
      "source": [
        "# Линейный классификатор Softmax \n",
        "\n",
        "Для выполнения этого задания нужно будет дописать код в этом ноутбуке  \n",
        "\n",
        "В этом упражнении Вам предстоит:\n",
        "\n",
        "- реализовать функцию потерь (**loss**) для Softmax классификатора\n",
        "- реализовать векторизованную функцию для вычисления **аналитического градиента**\n",
        "- **оптимизировать** матрицу весов W с помощью стохастического градиентного спуска **SGD**\n",
        "- найти лучшие гиперпараметры **learning rate и regularization** \n",
        "- **визуализировать** матрицу оптимальных весов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "aZFniEy8DAuc"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (16.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI6Y5fHiJx9e"
      },
      "source": [
        "# Загрузка датасета CIFAR-10 и предварительная подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "065Yd2AjDAud"
      },
      "source": [
        "# Очистим значения переменных, чтобы избежать проблем с излишним потреблением памяти\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Проверим размер входных и выходных векторов.\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLmw-REjJ8Do"
      },
      "source": [
        "# Перед началом работы полезно посмотреть на данные.\n",
        "# Отобразим пример из каждого класса.\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(X_train[idx].astype('uint8'))\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1MTg2q5KDZw"
      },
      "source": [
        "# Для удобства преобразуем двумерные изображения в одномерные вектора fp64\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1)).astype(np.float64)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1)).astype(np.float64)\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "# Проверим размер полученных данных\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Training label shape: ', y_train.shape)\n",
        "print('Test label shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5NPvm_fKG9o"
      },
      "source": [
        "# Нормализуем значения яркости пикселей \n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) \n",
        "\n",
        "# визуализируем среднюю яркость\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
        "plt.show()\n",
        "\n",
        "# Вычтем средние значения яркости\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "\n",
        "# Добавим к вектору исходных данных фиктивный признак с постоянным значением 1.\n",
        "# Этот трюк позволит избежать лишних вычислений: x @ W + b  => x' @ W'\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gOvIlepPDAug"
      },
      "source": [
        "## Softmax Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KH17iI9Gowa"
      },
      "source": [
        "# Реализуйте функцию вычисления softmax loss и dW с помощью циклов\n",
        "\n",
        "def softmax_loss_naive(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Hbd-SlbFDAuh"
      },
      "source": [
        "# Инициализируем веса значениями близкими, но не равными нулю\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_test, y_test, 0.0)\n",
        "\n",
        "# Обязательно проверим правильность реализации функции\n",
        "# Для 10 классов loss должен быть около -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('Начальное значение Loss = %f' % (-np.log(0.1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa6KyB0yHnkk"
      },
      "source": [
        "**Вопрос 1**\n",
        "\n",
        "Объясните, почему мы предположили, что для 10 классов при весах близких к нулю\n",
        " softmax loss приблизительно равен -log(0.1)?  \n",
        "$\\color{blue}{\\textit Ответ:}$ *заполнить здесь* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuning",
        "tags": [
          "code"
        ]
      },
      "source": [
        "# Теперь реализуйте более эффективную векторизованную версию \n",
        "# функции вычисления loss и dW\n",
        "\n",
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function, vectorized version.\n",
        "    Inputs and outputs are the same as softmax_loss_naive.\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_2PIGn3HvzK"
      },
      "source": [
        "# Оценим, насколько быстрее стали вычисления\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_test, y_test, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_test, y_test, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# Для оценки близости двух матриц градиентов используем норму Фробениуса\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94MroGpUK1_t"
      },
      "source": [
        "# Стохастический градиентный спуск"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yF-MdGCK8lZ"
      },
      "source": [
        "# Реализуйте SGD и проверьте результат \n",
        "\n",
        "class SoftmaxClassifier():\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "              batch_size=32, verbose=True):\n",
        "        \"\"\"\n",
        "        Обучение классификатора с помощью стохастического градиентного спуска\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss,  grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f '  % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkYWqdb2K1St"
      },
      "source": [
        "softmax_cls = SoftmaxClassifier()\n",
        "tic = time.time()\n",
        "loss_hist = softmax_cls.train(X_train[:1000], y_train[:1000], learning_rate=1e-4, reg=1e-3,\n",
        "                      num_iters=1000, verbose=True)\n",
        "toc = time.time()\n",
        "print('That took %fs' % (toc - tic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIAQM6c-MkoH"
      },
      "source": [
        "# Построим график зависимости loss от количества итераций\n",
        "plt.plot(loss_hist)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxoJZ186Mvgv"
      },
      "source": [
        "# оцените точность предсказания на выборках train и val\n",
        "y_train_pred = softmax_cls.predict(X_train)\n",
        "print('training accuracy: %f' % (np.mean(y_train == y_train_pred) ))\n",
        "y_test_pred = softmax_cls.predict(X_test)\n",
        "print('validation accuracy: %f' % (np.mean(y_test == y_test_pred) ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwv4sTdNEvU"
      },
      "source": [
        "# Поиск лучших гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLB88C5BM-_D"
      },
      "source": [
        "# Используйте валидационную выборку для выбора лучших гиперпараметров \n",
        "# (learning rate and regularization strength)\n",
        "# Добейтесь точности не меньше 0.38 на выборке test.\n",
        "# Используйте словарь results в котором \n",
        "# ключи - кортеж  (learning_rate, regularization_strength)\n",
        "# значения - (training_accuracy, validation_accuracy)\n",
        "# Точность вычисляется как отношение числа верно предсказанных классов \n",
        "# к объему выборки\n",
        "results = {}\n",
        "best_val = -1   \n",
        "best_softmax = None # Лучший экземпляр Softmax classifier \n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Напишите код, позволяющий найти лучшее значение гиперпараметров на val       #\n",
        "# выборке. Для каждой комбинации гиперпараметров обучите классификатор         #\n",
        "# на train выборке, вычислите точность на выборках train, val и сохраните      #\n",
        "# результат в словарь results. Лучшее значение точности сохраните в best_val   #\n",
        "# лучший классификатор - в best_softmax                                        #\n",
        "#                                                                              #\n",
        "# Подсказка: чтобы уменьшить время обучения, используйте dev подвыборку.       #\n",
        "# После того, как код будет реализован и отлажен, замените dev на train.       #\n",
        "################################################################################\n",
        "\n",
        "# Пример списка допустимых значений. Можете изменить на свое усмотрение.\n",
        "learning_rates = [1e-7, 5e-5]\n",
        "regularization_strengths = [2.5e4, 5e4]\n",
        "\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "# Вывод результатов.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9amiJoBNDIC"
      },
      "source": [
        "# Визуализируем результаты кросс-валидации\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "# pdb.set_trace()\n",
        "\n",
        "x_scatter = [math.log10(x[0]) for x in results]\n",
        "y_scatter = [math.log10(x[1]) for x in results]\n",
        "\n",
        "# график accuracy на обучении\n",
        "marker_size = 100\n",
        "colors = [results[x][0] for x in results]\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.tight_layout(pad=3)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 training accuracy')\n",
        "\n",
        "# график accuracy на валидации\n",
        "colors = [results[x][1] for x in results] # default size of markers is 20\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
        "plt.colorbar()\n",
        "plt.xlabel('log learning rate')\n",
        "plt.ylabel('log regularization strength')\n",
        "plt.title('CIFAR-10 validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "ghx3RGPPDAuk"
      },
      "source": [
        "# Точность на test выборке\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFONVgKYr5bS"
      },
      "source": [
        "y_test_pred = softmax_cls.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap4ZunISDAul"
      },
      "source": [
        "# Визуализируем веса W для каждого класса\n",
        "w = softmax_cls.W[:-1,:] # отбросим фиктивное измерение bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Масштабируем веса в значения от 0 до 255 для визуализации\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'), interpolation='quadric')\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6odQCbWQDAum"
      },
      "source": [
        "# ПОЗДРАВЛЯЕМ!\n",
        "\n",
        "Это конец задания. Не забудьте сохранить этот ноутбук со всеми выводами из ячеек и отправить на почту ailabintsev@fa.ru"
      ]
    }
  ]
}